import os
import re

import js2py  # ç”¨äºæ‰§è¡Œ JS è§£å¯†ä»£ç 
import requests
from bs4 import BeautifulSoup
from lzstring import LZString

HEADERS = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.manhuagui.com/"}


def get_img_data_urls(chapter_url):
    resp = requests.get(chapter_url, headers=HEADERS)
    soup = BeautifulSoup(resp.text, "html.parser")
    script = soup.find("script", text=re.compile(r"img_data"))
    if not script:
        raise Exception("æœªæ‰¾åˆ° img_data è„šæœ¬")
    base64_str = re.search(r'img_data\s*=\s*"([^"]+)"', script.string).group(1)
    images = LZString().decompressFromBase64(base64_str).split("|")
    chapter_url_cur = chapter_url
    return [requests.compat.urljoin(chapter_url_cur, img) for img in images]


def download_images(img_urls, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    session = requests.Session()
    session.headers.update(HEADERS)
    for idx, url in enumerate(img_urls, 1):
        # å¼ºåˆ¶ä½¿ç”¨ .png æˆ– .jpg åç¼€
        ext = ".png" if url.lower().endswith(".png.webp") else ".jpg"
        fname = f"{idx:03d}{ext}"
        out = os.path.join(save_dir, fname)
        r = session.get(url, stream=True)
        if r.status_code == 200:
            with open(out, "wb") as f:
                for chunk in r.iter_content(1024):
                    f.write(chunk)
            print(f"âœ… ä¸‹è½½: {fname}")
        else:
            print(f"âš ï¸ é”™è¯¯çŠ¶æ€ {r.status_code}: {url}")


def crawl_chapter(base_url, chapter_base_url, save_root):
    page = 1
    while True:
        url = f"{chapter_base_url.split('#')[0]}?p={page}"
        try:
            img_urls = get_img_data_urls(url)
        except Exception as e:
            print("ğŸ”š ç»“æŸï¼Œæ— æ³•è·å–ç¬¬", page, "é¡µ:", e)
            break
        save_dir = os.path.join(save_root, f"page_{page:03d}")
        print("â¡ï¸ é¡µé¢", page, "å…±", len(img_urls), "å¼ å›¾ç‰‡")
        download_images(img_urls, save_dir)
        page += 1


if __name__ == "__main__":
    BASE = "https://www.manhuagui.com/comic/1147"
    CHAPTER = "https://www.manhuagui.com/comic/1147/10498.html"
    crawl_chapter(BASE, CHAPTER, "downloaded_chapter")
